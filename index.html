<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Prof. Saenko's Research Group</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">Prof. Saenko's Research Group</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="people.html">People</a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=9xDADY4AAAAJ&hl=en">Publications</a>
                    </li>
                    <li>
                        <a href="teaching.html">Teaching</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Research <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            <li>
                                <a href="projects.html">Projects</a>
                            </li>
                            <li>
                                <a href="https://github.com/VisionLearningGroup">GitHub</a>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <a href="ksaenko.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

		<div class="row">
		<!-- News -->

            <div class="col-lg-6">

	<h2 class="page-header">News</h2>
		<ul>
			<li> <a href="https://drive.google.com/open?id=1s_5AwOHfRCWvHesWBfCUkbEJ8dqBmNdR"> Slides (PDF) </a> 
				from Kate Saenko's talk at the 
				<a href="https://sites.google.com/view/learning-with-limited-data/schedule?authuser=0">
					ICCV 2019 Tutorial on Learning with Limited Labels</a> 
			</li>
			<li> Our dataset<a href="http://ai.bu.edu/DomainNet/"> DomainNet </a> has now been released. The dataset contains 6 domains, 345 categories and about 0.6 million images, which is by far the largest cross-domain dataset.</li>
             <li>Our paper <a href="http://proceedings.mlr.press/v97/peng19b/peng19b.pdf">Domain Agnostic Learning with Disentangled Representations</a> was accepted to ICML2019 as a <b>Long Oral</b> paper</li>
            <li>Our paper <a href="https://arxiv.org/pdf/1812.01754.pdf">Moment Matching for Multi-Source Domain Adaptation</a> was accepted to ICCV2019 as an <b>Oral</b> paper</li>
            <li> Our paper <a href="https://arxiv.org/pdf/1812.04798.pdf">Strong-Weak Distribution Alignment for Adaptive Object Detection</a> was accepted to CVPR 2019</li>
			<li> Several new papers on uncovering bias in captioning models (<a href="https://arxiv.org/abs/1807.00517">ECCV'18</a>,
				<a href="https://arxiv.org/abs/1809.02156">EMNLP'18</a>), explainable AI (<a href="https://arxiv.org/abs/1806.07421">BMVC'18</a>,
				<a href="https://arxiv.org/abs/1807.08556">ECCV'18</a>) and language-based navigation (<a href=""https://arxiv.org/abs/1806.02724">NIPS'18)</a>
			</li>
			<li> Introducing High School Women to the World of Artificial Intelligence <br>
					<a href="http://www.bu.edu/today/2018/boston-university-ai4all/">
			</li>


			</li>
			<li> Our paper <a href="https://openreview.net/pdf?id=HJIoJWZCZ"> Adversarial Dropout Regularization </a> was accepted to ICLR 2018.
			<li> Code of the ICCV 2017 paper <a href="./r-c3d/index.html"> R-C3D for action detection in video</a> is released in the group's <a href="https://github.com/VisionLearningGroup/R-C3D">github repository</a>.
			</li>
			<li> Our paper <a href="arxiv.org/abs/1707.04046"> Stable Distribution Alignment Using the Dual of the Adversarial Distance </a> was accpeted to WICLR 2018.
			<li> Two papers accepted to ICCV 2017: <a href="https://arxiv.org/abs/1704.05526"> Learning to Reason (End-to-end Neural Modules) </a>
				and <a href="https://arxiv.org/abs/1703.07814"> R-C3D for action detection in video</a>.
			</li>
				<li>
				     Our <a href="http://www.ulrichviereck.com/CoRL2017">paper on robots learning to grasp</a> is accepted to CoRL 2017. <br>
					 <iframe width="320"  height="180" src="https://www.youtube.com/embed/ZGzlLc9bg1A" frameborder="0" allowfullscreen></iframe> <br>
				</li>
				<li>
					<p>The <a href="http://ai.bu.edu/visda-2017/">VisDA-2017 Challenge winners</a> have been announced on October 10.
					 Join us at the <a href="http://adas.cvc.uab.es/task-cv2017/"> TASK-CV workshop </a> to find out more!</p>
				</li>

				<li> Slides from Kate's talk at the <a href="https://sites.google.com/site/nips2016intelligenttrans/schedule">NIPS 2016 workshop on Machine Learning for Intelligent
				  Transportation Systems</a> on December 9,
				  <a href="https://drive.google.com/file/d/0B4IapRTv9pJ1czJkQUpMSWpLSVU/view">Domain Adaptation for Perception and Action.</a>
				</li>
				<li> We are hosting the <a href="http://vision.cs.uml.edu/necv2016.html"> 2016 New England Computer Vision Workshop (NECV)</a> at Boston University.
				</li>
				<li> Our paper titled Deep CORAL: Correlation Alignment for Deep Domain Adaptation has won the <a href="/pubs/awardTASCK-CV2016.pdf"> <font color="red">Honorable Mention Paper</font></a> of the TASK-CV workshop at ECCV'2016.
				</li>
				<li>Our paper
					<a href="http://arxiv.org/abs/1511.05234">"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering"</a>
					is accepted to ECCV 2016. Here is the video spotlight:<br><br>
					<iframe width="320" height="180" src="https://www.youtube.com/embed/FjpRwVKYJQ8?rel=0" frameborder="0" allowfullscreen></iframe>
					<br><br>
				</li>
				<li>Our paper <a href="https://arxiv.org/abs/1609.04356">Combining Texture and Shape Cues for Object Detection with
				Minimal Supervision</a> was accepted to ACCV-16.
				</li>
				<li>Our paper
					<a href="https://arxiv.org/abs/1605.06695">"Fine-to-coarse Knowledge Transfer For Low-Res Image Classification"</a>
					was accepted to ICIP-16.
				</li>
				<li> <font color="red">Moving to Boston University!</font> Kate has accepted an Assistant Professor position in the
				<a href="http://www.bu.edu/cs/">Computer Science Department</a> at <a href="http://www.bu.edu/">Boston University</a>, and will be moving this summer, along with her group. Stay tuned for the new website.
				</li>
				<li> <a href="https://arxiv.org/abs/1607.01719">Deep CORAL: Correlation Alignment for Deep Domain Adaptation (Extended Abstract)</a>.
				</li>
				<li> Two orals accepted to <a href="http://cvpr2016.thecvf.com/">CVPR 2016</a>:
				<a href="http://arxiv.org/abs/1511.05284">Deep Compositional Captioning: Describing Novel Object Categories
				without Paired Training Data</a> and <a href="http://arxiv.org/abs/1511.04164">Natural Language Object Retrieval</a>.
				</li>
				<li>  Slides from Kate's MIT talk on March 15th:
				<a href="https://drive.google.com/file/d/0B4IapRTv9pJ1eWowSzJBNGh0aFU/view?usp=sharing">Adaptive Deep Learning for
				Vision and Language.</a>
				</li>
				<li>  Our paper titled Return of Frustratingly Easy Domain Adaptation (Extended Abstract) has won the <a href="/pubs/awardTASCK-CV2015-Best.pdf"> <font color="red">Best Paper Prize</font></a> of the TASK-CV workshop at ICCV'2015.
				</li>
				<li>  Our paper titled <a href="http://arxiv.org/abs/1511.05547">Return of Frustratingly Easy Domain Adaptation</a> was accepted to AAAI-16.
				</li>
				<li> Our group is co-organizing the <a href="https://sites.google.com/site/tlworkshop2015/">Transfer and Multi-Task Learning: Trends and New Perspectives Workshop</a> at NIPS 2015 on December 12th, 2015 in Montreal, Canada.
				</li>
				<li> Four papers accepted to ICCV 2015! Here are some spotlights:<br><br>
					<iframe width="320" height="180"  src="https://www.youtube.com/embed/-xNI7e7YgDk?rel=0" frameborder="0" allowfullscreen></iframe>
				    <iframe width="320" height="180"  src="https://www.youtube.com/embed/gQMDX1Q2OfY?rel=0" frameborder="0" allowfullscreen></iframe>
				<br><br>
				</li>
				<li> Slides from Kate's lecture at the <a href="">Microsoft Machine Learning and Intelligence School</a>, which took place in St Petersburg, Russia, are available here:
					<a href="https://drive.google.com/file/d/0B4IapRTv9pJ1V1N1LUFmUVRDbWc/view?usp=sharing">part1</a>, <a href="https://drive.google.com/file/d/0B4IapRTv9pJ1MmlSc0NtSXdmMmc/view?usp=sharing">part2</a>.
				</li>
				<li> We will present <a href="pubs/cvpr2015_workshop_virtual_dataset.pdf">Generating Large Scale Datasets from 3D CAD Models</a> at the <a href="https://sites.google.com/site/cvpr2015futureofdataworkshop/schedule">workshop</a>, initial datasets are available <a href="http://vision.cs.uml.edu/datasets/">here</a>
				</li>
				<li> Kate is co-organizing the <a href="https://sites.google.com/site/cvpr2015futureofdataworkshop/schedule">Future of Datasets in Vision Workshop</a> at CVPR 2015 on June 11th, 2015 in Boston, MA.
				</li>
				<li> <a href="https://drive.google.com/open?id=0B4IapRTv9pJ1b0xObHVsMmxieHc">Slides</a> from my recent <a href="http://opendatascicon.com/schedule/deep-learning/">tutorial</a> on the deep learning library <a href="caffe.berkeleyvision.edu">Caffe</a> at the <a href="http://opendatascicon.com/">Open Data Science Conference</a> on May 30th in Boston.
				</li>
				<li> <a href="http://lsda.berkeleyvision.org">Large-Scale Detection by Adaptation</a> 7K Category Detection models are now available!
				</li>
				<li> Our paper titled <a href="/pubs/bmvc14_sun_fromvirtualtoreal.pdf">From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains</a> was accepted to BMVC 2014. See also <a href="bmvc14_FromVirtualToReality.pptx">slides</a> from a recent talk.
				</li>
				<li>
					<a href="./projects.html#data">DeCAF features</a> that achieve the state of the art on the Office domain adaptation dataset are now available for download.
				</li>
				<li> Kate is co-chairing the <a href="http://www.cvc.uab.es/adas/task-cv2014">TASK-CV Workshop on Transferring and Adapting Source Knowledge in Computer Vision</a>, co-located with ECCV2014.</li>
				<li> Kate will be giving a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.360.3572&rep=rep1&type=pdf">tutorial on Domain Transfer Learning for Vision Applications</a> at
					<a href="http://tab.computer.org/pamitc/archive/cvpr2012/program-details/tutorials.html"> CVPR 2012</a> with Dong Xu and Ivor Tsang.</li>
				<li> Kate is co-organizing the <a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2539"> Workshop on Integrating Language and Vision</span></a>, held at NIPS 2011 in Grenada, Spain.</li>

			</ul>
			</div>

			<div class="col-lg-5">
			<a class="twitter-timeline" href="https://twitter.com/kate_saenko_">Twitter</a>
			<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
			</div>

		</div>

		<!-- Footer -->
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Kate Saenko 2017</p>
                </div>
            </div>
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Script to Activate the Carousel -->
    <script>
    $('.carousel').carousel({
        interval: 5000 //changes the speed
    })
    </script>

</body>

</html>
